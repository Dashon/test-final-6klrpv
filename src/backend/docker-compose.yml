version: '3.8'

# Define named volumes for persistent data
volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      device: /data/postgres
      o: bind
  mongo_data:
    driver: local
  redis_data:
    driver: local
  elasticsearch_data:
    driver: local

# Define networks with encryption
networks:
  backend:
    driver: bridge
    driver_opts:
      encrypted: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16

# Service definitions
services:
  # API Gateway
  api-gateway:
    build:
      context: ./api-gateway
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8443:8443"
      - "8444:8444"
    environment:
      - KONG_DATABASE=off
      - KONG_DECLARATIVE_CONFIG=/usr/local/kong/declarative/kong.yml
      - KONG_PROXY_ACCESS_LOG=/dev/stdout
      - KONG_ADMIN_ACCESS_LOG=/dev/stdout
      - KONG_PROXY_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_ERROR_LOG=/dev/stderr
      - KONG_PROXY_LISTEN=0.0.0.0:8000 ssl
      - KONG_ADMIN_LISTEN=0.0.0.0:8001 ssl
      - KONG_SSL_CERT=/usr/local/kong/ssl/server.crt
      - KONG_SSL_CERT_KEY=/usr/local/kong/ssl/server.key
    volumes:
      - ./api-gateway/config:/usr/local/kong/declarative
      - ./api-gateway/ssl:/usr/local/kong/ssl
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Booking Service
  booking-service:
    build:
      context: ./booking-service
      dockerfile: Dockerfile
    environment:
      - NODE_ENV=production
      - PORT=3000
      - DB_HOST=postgres
      - REDIS_HOST=redis
      - AMADEUS_API_KEY=${AMADEUS_API_KEY}
      - AMADEUS_API_SECRET=${AMADEUS_API_SECRET}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G

  # PostgreSQL Database
  postgres:
    image: postgres:15.3-alpine
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=travel_platform
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G

  # Redis Cache
  redis:
    image: redis:7.0.11-alpine
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G

  # MongoDB for Social Features
  mongodb:
    image: mongo:6.0.7
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_USER}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_PASSWORD}
    volumes:
      - mongo_data:/data/db
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G

  # Elasticsearch for Search
  elasticsearch:
    image: elasticsearch:8.7.1
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q 'status.*green'"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:v2.44.0
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=15d'
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:9.5.2
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G

  # Fluentd Logging
  fluentd:
    image: fluent/fluentd:v1.16.1-debian-1.0
    volumes:
      - ./logging/fluentd.conf:/fluentd/etc/fluent.conf
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M